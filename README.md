# CapstoneTemporary
Conversational Loop integrated with agent.py:

Instantiation and Session Management:
When a new chat session starts, a Memory instance should be created. This instance will be unique to that session.
The Memory instance will be passed to or accessed by the Perception and Decision modules as needed.
Check for session end command, Check for repeated question in Memory, 

Perception Module Change: 
The Perception module's role might shift slightly. While it could still do initial NLP (like entity extraction if needed for very specific, non-LLM handled tasks), its main role in a conversational LLM-centric agent would be:
Receiving User Input: Getting the raw text.

Memory Module Change: 
Accessing Relevant History (Optional Pre-filtering): It could potentially do a very quick check in memory for exact repeat questions or simple commands before passing to the more expensive Decision LLM. However, your Memory.check_for_repeated_question already handles this.
Loop : 
Formatting Input for Decision: Ensuring the user query and any immediately relevant, short-term context (like the very last turn) are packaged.
Decision Module (module/decision.py) Changes
This module will see significant changes. It needs to:
Accept Memory Context: Its main execution method will now take the user_query and the relevant_context_from_memory (and potentially the full conversation history or a summary).
Formulate Prompts with Memory: The prompt engineered for the LLM must now incorporate this memory context effectively. This is crucial.
The prompt should instruct the LLM to consider the provided memory (facts, current scheme, recent conversation) when making its decision.
It should also be instructed to indicate if information needs to be stored or updated in memory (e.g., "User wants to finalize scheme X," or "Store Y as a new fact").
Interpret LLM Output for Memory Operations: The LLM's response might contain not just the next tool to call or a final answer, but also implicit or explicit instructions to update memory. The Decision module (or a helper function) will need to parse these.
Handle "Memory Retrieval" as an Action: Sometimes the LLM's best action is to explicitly state it's using memory. The prompt should allow for this.
Iterative Decision Making: If a tool is executed, the Decision module might be called again with the tool's output to synthesize a final human-readable response, as shown in the example loop.

Proposed implementation plan ( generated by gemini) does seem legit to me:
Part 1: Core Memory Integration & Basic Conversational Loop
Step 1.1: Implement and Test module/memory.py Standalone
Changes:
Finalize the Memory class as we've discussed (module/memory.py).
Ensure all methods (add_conversation_turn, store_fact, retrieve_fact, store_scheme_data, retrieve_scheme_data, set_current_scheme, get_current_scheme_id, get_current_scheme_data, check_for_repeated_question, get_relevant_context_for_decision, store_generated_schemes, etc.) are implemented.
Testing:
Write unit tests for module/memory.py.
Test each method in isolation:
Can you store and retrieve facts correctly?
Can you store and retrieve scheme data?
Does set_current_scheme work as expected?
Does add_conversation_turn store history correctly?
Does check_for_repeated_question identify repeats?
Does get_relevant_context_for_decision produce a reasonable string (even if basic for now)?
Does store_generated_schemes and its retrieval methods work?
Goal: A robust, independently verifiable Memory module.
Step 1.2: Create the Basic ConversationalAgent and Integrate Memory
Changes:
Create the ConversationalAgent class (likely in main.py or a new agent.py).
In its __init__, instantiate Memory.
Implement a basic conversational loop in process_user_query:
Agent says "What do you want to do today?"
User provides input.
Store user input in memory using memory.add_conversation_turn().
Implement memory.check_for_repeated_question(). If a repeat, respond from memory.
For now, if not a repeat, the agent can have a placeholder response like "I received your query: [user_query]. I'm still learning how to process this fully."
Store agent's placeholder response in memory.
Implement basic session end (e.g., on "bye").
Testing:
Manually interact with the agent.
Verify that conversation history is being stored in the Memory instance.
Test the repeated question functionality.
Test session end.
Inspect the Memory object's state after a few turns.
Goal: A basic loop where user input and agent responses are captured in memory, and repeat questions are handled.
Part 2: Integrating Decision with Memory and Basic Tool Use (No Chaining Yet)
Step 2.1: Adapt Decision Module to Use Memory Context
Changes:
Modify Decision.execute_plan_with_memory to accept memory_context and full_conversation_history as discussed.
Update the system prompt in Decision to instruct the LLM on how to use the provided memory_context and conversation_history.
Ensure the LLM is prompted to output thought, tool_name, tool_input, speak, and memory_actions.
Testing:
Unit tests for Decision (can be mocked initially):
Provide sample user_query, memory_context, and history.
Verify that the prompt constructed for the LLM includes these elements correctly.
Mock the LLM's response and test the parsing of the JSON output (thought, tool, input, speak, memory_actions).
Goal: Decision module can formulate prompts incorporating memory and parse structured LLM responses.
Step 2.2: Enhance ConversationalAgent to Use Decision and Execute Simple Tools
Changes:
In ConversationalAgent.process_user_query:
Call memory.get_relevant_context_for_decision().
Pass user_query, memory_context, and history to decision.execute_plan_with_memory().
Implement logic to execute a single, simple tool (e.g., addition or a mock tool) if tool_name is returned by Decision.
After tool execution, for now, the tool's direct output can be the final_answer. (We'll add the re-evaluation step later).
Implement basic processing of memory_actions returned by Decision (e.g., store_fact, set_current_scheme).
Update memory.add_conversation_turn() with tool usage details and the final answer.
Testing:
Use a very simple tool (e.g., addition).
Manually interact:
Ask a question that should trigger the simple tool. Verify the tool is called and its output is returned.
Ask a question that should result in a final_answer without a tool.
Give a command that Decision should interpret as a memory_action (e.g., "Remember my favorite color is blue"). Verify the fact is stored in memory.
Check memory.conversation_history to see if tool calls and memory actions are logged.
Goal: Agent can use the Decision LLM, incorporate memory context, execute a single tool, and perform basic memory updates based on LLM instructions.
Part 3: Enhancing Perception and Integrating it
Step 3.1: Implement "Heavier" Perception Module
Changes:
Implement the enhanced Perception.process_user_input as discussed, including basic intent recognition (e.g., "finalize_scheme") and entity extraction (e.g., scheme IDs).
Testing:
Unit tests for module/perception.py:
Test various user inputs to ensure correct intent and entities are extracted.
Test edge cases and variations in phrasing.
Goal: A Perception module that can reliably extract predefined intents and entities.
Step 3.2: Integrate Enhanced Perception into ConversationalAgent and Decision
Changes:
In ConversationalAgent.process_user_query:
Call perception.process_user_input() first.
Handle any direct intents from Perception (e.g., "end_session", or simple "finalize_scheme" if we choose to handle it before Decision).
Pass the perception_data to decision.execute_plan_with_memory().
In Decision.execute_plan_with_memory:
Modify the method to accept perception_data.
Update the LLM prompt to include this pre-processed information and instruct the LLM on how to use it alongside the raw query.
Testing:
Manually interact:
Test queries where Perception should extract intents/entities.
Observe if the Decision LLM's "thought" process indicates it's using the perception_data.
Verify that the overall agent behavior is consistent or improved.
For example, if Perception identifies "finalize scheme 3", does the Decision module correctly generate a memory_action to set the current scheme?
Goal: Perception assists Decision, potentially simplifying the LLM's task for certain queries.
Part 4: Advanced Tool Usage - Chaining and Re-evaluation
Step 4.1: Implement Tool-to-Tool Calling and Dependency Injection
Changes:
Refactor tool classes to accept memory and tools_execution_map dependencies via a set_dependencies method or constructor injection.
Implement an example of a tool (ToolA) calling another tool (ToolB), where ToolB might save data to memory, as per our ComplexCalculatorTool example.
Update ConversationalAgent to inject these dependencies into tools during initialization.
Testing:
Unit tests for the tools involved in chaining:
Test ToolB independently, ensuring it saves to a mock memory correctly.
Test ToolA, mocking ToolB and memory, to ensure it calls ToolB correctly and processes its output.
Integration test: In ConversationalAgent, trigger ToolA. Verify that ToolB is called, data is saved to memory (inspect self.memory), and ToolA produces the correct final output.
Goal: Tools can successfully call other tools and interact with memory.
Step 4.2: Implement Re-evaluation Loop in ConversationalAgent
Changes:
In ConversationalAgent.process_user_query, after a tool executes:
If a tool was called and produced output, construct a new query/prompt for the Decision module (e.g., "Based on the tool output [output], what is the final answer?").
Call decision.execute_plan_with_memory() again with this new prompt, the tool output, and updated memory context.
The final_answer from this second call becomes the agent's response.
Testing:
Manually interact with queries that require a tool.
Verify that after the tool runs, the Decision LLM is called again to synthesize the tool's output into a user-friendly response.
Check the "thought" process of the LLM in both calls to Decision.
Goal: Agent can use tool outputs to formulate more natural and contextually relevant final answers.
Part 5: Comprehensive Scenario Testing and Refinement
Step 5.1: End-to-End Scenario Testing
Changes:
Implement all the tools from your example chat flow (ai_form_schemer, search_2050_materials, multiplication, addition, division, subtraction, search_documents, create_markdown_report, mkdwn_to_pdf). Ensure they are correctly "registered" and dependencies are injected.
Testing:
Run through the entire example chat flow provided in the initial problem description, step by step.
At each step, verify:
Correct tool is called (or memory is retrieved).
Memory is updated correctly (facts, scheme data, current scheme).
The agent's final answer is as expected.
The get_relevant_context_for_decision provides useful, non-overwhelming context.
Test variations of the queries.
Test error conditions (e.g., tool fails, LLM returns unexpected format).
Goal: The agent successfully handles the entire target conversation flow and demonstrates robust memory management and tool use.
Step 5.2: Prompt Engineering and Context Management Refinement
Changes:
Based on testing, refine the system prompts for the Decision LLM.
Fine-tune Memory.get_relevant_context_for_decision to be more intelligent about what it includes/excludes to optimize for the LLM's context window and relevance.
Testing:
Observe LLM behavior with different prompt variations.
Monitor the length and content of the context string sent to the LLM.
Goal: Optimized LLM performance and efficient use of the context window.
